<?xml version="1.0" ?>
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
<!-- saved from url=(0017)http://localhost/ -->
<script language="JavaScript" src="../../displayToc.js"></script>
<script language="JavaScript" src="../../tocParas.js"></script>
<script language="JavaScript" src="../../tocTab.js"></script>
<link rel="stylesheet" type="text/css" href="../../scineplex.css">
<title></title>
<link rel="stylesheet" href="../../Active.css" type="text/css" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rev="made" href="mailto:" />
</head>

<body>



<ul id="index">
  <li><a href="#NAME">NAME</a></li>
  <li><a href="#SYNOPSIS">SYNOPSIS</a></li>
  <li><a href="#DESCRIPTION">DESCRIPTION</a></li>
  <li><a href="#ROBOTS.TXT">ROBOTS.TXT</a></li>
  <li><a href="#ROBOTS.TXT-EXAMPLES">ROBOTS.TXT EXAMPLES</a></li>
  <li><a href="#SEE-ALSO">SEE ALSO</a></li>
  <li><a href="#COPYRIGHT">COPYRIGHT</a></li>
</ul>

<h1 id="NAME">NAME</h1>

<p>WWW::RobotRules - database of robots.txt-derived permissions</p>

<h1 id="SYNOPSIS">SYNOPSIS</h1>

<pre><code><code> <span class="keyword">use</span> <span class="variable">WWW::RobotRules</span><span class="operator">;</span>
 <span class="keyword">my</span> <span class="variable">$rules</span> <span class="operator">=</span> <span class="variable">WWW::RobotRules</span><span class="operator">-&gt;</span><span class="variable">new</span><span class="operator">(</span><span class="string">'MOMspider/1.0'</span><span class="operator">);</span>
 
 <span class="keyword">use</span> <span class="variable">LWP::Simple</span> <span class="string">qw(get)</span><span class="operator">;</span>
 
 <span class="operator">{</span>
   <span class="keyword">my</span> <span class="variable">$url</span> <span class="operator">=</span> <span class="string">"http://some.place/robots.txt"</span><span class="operator">;</span>
   <span class="keyword">my</span> <span class="variable">$robots_txt</span> <span class="operator">=</span> <span class="variable">get</span> <span class="variable">$url</span><span class="operator">;</span>
   <span class="variable">$rules</span><span class="operator">-&gt;</span><span class="variable">parse</span><span class="operator">(</span><span class="variable">$url</span><span class="operator">,</span> <span class="variable">$robots_txt</span><span class="operator">)</span> <span class="keyword">if</span> <span class="keyword">defined</span> <span class="variable">$robots_txt</span><span class="operator">;</span>
 <span class="operator">}</span>
 
 <span class="operator">{</span>
   <span class="keyword">my</span> <span class="variable">$url</span> <span class="operator">=</span> <span class="string">"http://some.other.place/robots.txt"</span><span class="operator">;</span>
   <span class="keyword">my</span> <span class="variable">$robots_txt</span> <span class="operator">=</span> <span class="variable">get</span> <span class="variable">$url</span><span class="operator">;</span>
   <span class="variable">$rules</span><span class="operator">-&gt;</span><span class="variable">parse</span><span class="operator">(</span><span class="variable">$url</span><span class="operator">,</span> <span class="variable">$robots_txt</span><span class="operator">)</span> <span class="keyword">if</span> <span class="keyword">defined</span> <span class="variable">$robots_txt</span><span class="operator">;</span>
 <span class="operator">}</span>
 
 <span class="comment"># Now we can check if a URL is valid for those servers</span>
 <span class="comment"># whose "robots.txt" files we've gotten and parsed:</span>
 <span class="keyword">if</span><span class="operator">(</span><span class="variable">$rules</span><span class="operator">-&gt;</span><span class="variable">allowed</span><span class="operator">(</span><span class="variable">$url</span><span class="operator">))</span> <span class="operator">{</span>
     <span class="variable">$c</span> <span class="operator">=</span> <span class="variable">get</span> <span class="variable">$url</span><span class="operator">;</span>
     <span class="operator">...</span>
 <span class="operator">}</span>
</code></code></pre>

<h1 id="DESCRIPTION">DESCRIPTION</h1>

<p>This module parses <i>/robots.txt</i> files as specified in &quot;A Standard for Robot Exclusion&quot;, at &lt;http://www.robotstxt.org/wc/norobots.html&gt; Webmasters can use the <i>/robots.txt</i> file to forbid conforming robots from accessing parts of their web site.</p>

<p>The parsed files are kept in a WWW::RobotRules object, and this object provides methods to check if access to a given URL is prohibited. The same WWW::RobotRules object can be used for one or more parsed <i>/robots.txt</i> files on any number of hosts.</p>

<p>The following methods are provided:</p>

<dl>

<dt id="rules-WWW::RobotRules-new-robot_name-">$rules = WWW::RobotRules-&gt;new($robot_name)</dt>
<dd>

<p>This is the constructor for WWW::RobotRules objects. The first argument given to new() is the name of the robot.</p>

</dd>
<dt id="rules-parse-robot_txt_url-content-fresh_until-">$rules-&gt;parse($robot_txt_url, $content, $fresh_until)</dt>
<dd>

<p>The parse() method takes as arguments the URL that was used to retrieve the <i>/robots.txt</i> file, and the contents of the file.</p>

</dd>
<dt id="rules-allowed-uri-">$rules-&gt;allowed($uri)</dt>
<dd>

<p>Returns TRUE if this robot is allowed to retrieve this URL.</p>

</dd>
<dt id="rules-agent-name-">$rules-&gt;agent([$name])</dt>
<dd>

<p>Get/set the agent name. NOTE: Changing the agent name will clear the robots.txt rules and expire times out of the cache.</p>

</dd>
</dl>

<h1 id="ROBOTS.TXT">ROBOTS.TXT</h1>

<p>The format and semantics of the &quot;/robots.txt&quot; file are as follows (this is an edited abstract of &lt;http://www.robotstxt.org/wc/norobots.html&gt;):</p>

<p>The file consists of one or more records separated by one or more blank lines. Each record contains lines of the form</p>

<pre><code><code>  &lt;field-name&gt;: &lt;value&gt;</code></code></pre>

<p>The field name is case insensitive. Text after the &#39;#&#39; character on a line is ignored during parsing. This is used for comments. The following &lt;field-names&gt; can be used:</p>

<dl>

<dt id="User-Agent">User-Agent</dt>
<dd>

<p>The value of this field is the name of the robot the record is describing access policy for. If more than one <i>User-Agent</i> field is present the record describes an identical access policy for more than one robot. At least one field needs to be present per record. If the value is &#39;*&#39;, the record describes the default access policy for any robot that has not not matched any of the other records.</p>

<p>The <i>User-Agent</i> fields must occur before the <i>Disallow</i> fields. If a record contains a <i>User-Agent</i> field after a <i>Disallow</i> field, that constitutes a malformed record. This parser will assume that a blank line should have been placed before that <i>User-Agent</i> field, and will break the record into two. All the fields before the <i>User-Agent</i> field will constitute a record, and the <i>User-Agent</i> field will be the first field in a new record.</p>

</dd>
<dt id="Disallow">Disallow</dt>
<dd>

<p>The value of this field specifies a partial URL that is not to be visited. This can be a full path, or a partial path; any URL that starts with this value will not be retrieved</p>

</dd>
</dl>

<p>Unrecognized records are ignored.</p>

<h1 id="ROBOTS.TXT-EXAMPLES">ROBOTS.TXT EXAMPLES</h1>

<p>The following example &quot;/robots.txt&quot; file specifies that no robots should visit any URL starting with &quot;/cyberworld/map/&quot; or &quot;/tmp/&quot;:</p>

<pre><code><code>  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  Disallow: /tmp/ # these will soon disappear</code></code></pre>

<p>This example &quot;/robots.txt&quot; file specifies that no robots should visit any URL starting with &quot;/cyberworld/map/&quot;, except the robot called &quot;cybermapper&quot;:</p>

<pre><code><code>  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space

  # Cybermapper knows where to go.
  User-agent: cybermapper
  Disallow:</code></code></pre>

<p>This example indicates that no robots should visit this site further:</p>

<pre><code><code>  # go away
  User-agent: *
  Disallow: /</code></code></pre>

<p>This is an example of a malformed robots.txt file.</p>

<pre><code><code>  # robots.txt for ancientcastle.example.com
  # I&#39;ve locked myself away.
  User-agent: *
  Disallow: /
  # The castle is your home now, so you can go anywhere you like.
  User-agent: Belle
  Disallow: /west-wing/ # except the west wing!
  # It&#39;s good to be the Prince...
  User-agent: Beast
  Disallow:</code></code></pre>

<p>This file is missing the required blank lines between records. However, the intention is clear.</p>

<h1 id="SEE-ALSO">SEE ALSO</h1>

<p><a href="../../lib/LWP/RobotUA.html">LWP::RobotUA</a>, <a href="../../lib/WWW/RobotRules/AnyDBM_File.html">WWW::RobotRules::AnyDBM_File</a></p>

<h1 id="COPYRIGHT">COPYRIGHT</h1>

<pre><code><code>  Copyright 1995-2009, Gisle Aas
  Copyright 1995, Martijn Koster</code></code></pre>

<p>This library is free software; you can redistribute it and/or modify it under the same terms as Perl itself.</p>


</body>

</html>


